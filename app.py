import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import base64
import re
from io import BytesIO, StringIO
import zipfile

# =========================
# PATCH PACK: ê³µí†µ ì„¤ì •Â·ìœ í‹¸ (ìë™ì‚½ì…)
# =========================
import numpy as np
import pandas as pd
from io import BytesIO
import zipfile

# === 1) ì„¤ì • ìƒìˆ˜(ë‹¨ì¼ ì¶œì²˜) ===
BASIS_MIN_PER_DAY = 480               # 1ì¼ ê¸°ì¤€ 480ë¶„
ABNORMAL_TASK_MIN = 600               # ë‹¨ì¼ ê¸°ë¡ 10ì‹œê°„ ì´ˆê³¼ëŠ” ë¶„ì„ì—ì„œ ì œì™¸(ì§„ë‹¨ìš© ì»·)
DEFAULT_ERROR_THRESHOLD_MIN = 480     # ì‘ì—…ì‹œê°„ ì˜¤ë¥˜ ì„ê³„(ê¸°ë³¸ 8ì‹œê°„, ì‚¬ì´ë“œë°”ì—ì„œ ë³€ê²½)
DISPLAY_DT_FMT = '%Y-%m-%d %H:%M'     # í‘œì‹œìš© ë‚ ì§œ í¬ë§·

# === 2) ë¬¸ìì—´ í‘œì¤€í™” + ë²”ì£¼í˜• ìºì‹± ===
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in ['íŒ€', 'ì‘ì—…ì', 'êµ¬ë¶„', 'ì¥ë¹„ID', 'ì¥ë¹„ëª…']:
        if c in df.columns:
            df[c] = df[c].astype(str).str.strip()
    if 'êµ¬ë¶„' in df.columns:
        df['êµ¬ë¶„'] = df['êµ¬ë¶„'].replace({
            'ì¥ì• /ì•ŒëŒ(AS)': 'ì¥ì• /ì•ŒëŒ',
            'ì‚¬ë¬´ì—…ë¬´ ': 'ì‚¬ë¬´ì—…ë¬´',
        })
    for col in ['ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    for c in ['íŒ€', 'êµ¬ë¶„', 'ì‘ì—…ì']:
        if c in df.columns:
            df[c] = df[c].astype('category')
    return df

# === 3) í‘œì‹œìš© ë‚ ì§œ í¬ë§· ì¼ì›í™” ===
def format_dt_display(df: pd.DataFrame) -> pd.DataFrame:
    disp = df.copy()
    for col in ('ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ'):
        if col in disp.columns:
            disp[col] = pd.to_datetime(disp[col], errors='coerce').dt.strftime(DISPLAY_DT_FMT)
    return disp

# === 4) ê²¹ì¹¨ ë³‘í•©(Union) â€” ì•ˆì „ ë£¨í”„ ë²„ì „ ===
def merge_union_minutes(group: pd.DataFrame):
    gg = group[['ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']].copy()
    gg['ì‹œì‘ì¼ì‹œ'] = pd.to_datetime(gg['ì‹œì‘ì¼ì‹œ'], errors='coerce')
    gg['ì¢…ë£Œì¼ì‹œ'] = pd.to_datetime(gg['ì¢…ë£Œì¼ì‹œ'], errors='coerce')
    gg = gg.dropna().sort_values('ì‹œì‘ì¼ì‹œ')
    if gg.empty:
        return pd.Series({'ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)': 0.0})
    total = pd.Timedelta(0)
    cur_s = gg.iloc[0]['ì‹œì‘ì¼ì‹œ']
    cur_e = gg.iloc[0]['ì¢…ë£Œì¼ì‹œ']
    for _, row in gg.iloc[1:].iterrows():
        s, e = row['ì‹œì‘ì¼ì‹œ'], row['ì¢…ë£Œì¼ì‹œ']
        if s <= cur_e:
            if e > cur_e: cur_e = e
        else:
            total += (cur_e - cur_s)
            cur_s, cur_e = s, e
    total += (cur_e - cur_s)
    return pd.Series({'ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)': total.total_seconds()/60})

# === 4-ALT) ê²¹ì¹¨ ë³‘í•© â€” ë²¡í„°í™”(ëŒ€ìš©ëŸ‰ ìµœì í™”) ===
def merge_union_minutes_fast(group: pd.DataFrame):
    gg = group[['ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']].copy()
    gg['ì‹œì‘ì¼ì‹œ'] = pd.to_datetime(gg['ì‹œì‘ì¼ì‹œ'], errors='coerce')
    gg['ì¢…ë£Œì¼ì‹œ'] = pd.to_datetime(gg['ì¢…ë£Œì¼ì‹œ'], errors='coerce')
    gg = gg.dropna().sort_values('ì‹œì‘ì¼ì‹œ')
    if gg.empty:
        return pd.Series({'ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)': 0.0})
    s = gg['ì‹œì‘ì¼ì‹œ'].values.astype('datetime64[ns]').astype('int64')
    e = gg['ì¢…ë£Œì¼ì‹œ' ].values.astype('datetime64[ns]').astype('int64')
    e_cummax = np.maximum.accumulate(e)
    is_new = np.empty(len(s), dtype=bool)
    is_new[0] = True
    is_new[1:] = s[1:] > e_cummax[:-1]
    grp = np.cumsum(is_new) - 1
    minutes = (pd.Series(e).groupby(grp).max() - pd.Series(s).groupby(grp).min()).sum() / 1e9 / 60.0
    return pd.Series({'ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)': float(minutes)})

# === 5) ëˆ„ë½í˜„í™© ê¸°ì´ˆ(ì¤‘ê°„ ì‚°ì¶œë¬¼) â€” ìºì‹œ ===
try:
    import streamlit as st
except Exception:
    class _Dummy: 
        def __getattr__(self, k): 
            def _f(*a, **kw): 
                return None
            return _f
    st = _Dummy()

@st.cache_data(show_spinner=False)
def build_presence_table(df: pd.DataFrame) -> pd.DataFrame:
    src = df.copy()
    if 'ì‘ì—…ì¼' not in src.columns:
        src['ì‘ì—…ì¼'] = pd.to_datetime(src['ì‹œì‘ì¼ì‹œ'], errors='coerce').dt.date
    workers = src[['íŒ€','ì‘ì—…ì']].dropna().drop_duplicates()
    date_range = pd.date_range(start=pd.to_datetime(src['ì‘ì—…ì¼']).min(),
                               end  =pd.to_datetime(src['ì‘ì—…ì¼']).max(),
                               freq='B').date
    idx = pd.MultiIndex.from_product([workers['ì‘ì—…ì'], date_range], names=['ì‘ì—…ì','ì‘ì—…ì¼'])
    all_rows = pd.DataFrame(index=idx).reset_index().merge(workers, on='ì‘ì—…ì', how='left')
    actual = src.groupby(['íŒ€','ì‘ì—…ì','ì‘ì—…ì¼']).size().rename('ì‘ì„±ì—¬ë¶€').reset_index()
    actual['ì‘ì„±ì—¬ë¶€'] = 1
    pres = all_rows.merge(actual, on=['íŒ€','ì‘ì—…ì','ì‘ì—…ì¼'], how='left').fillna({'ì‘ì„±ì—¬ë¶€':0})
    return pres

# === 6) ë‹¤ìš´ë¡œë“œ(ì—‘ì…€ ì—”ì§„ í´ë°± + ë¡œê¹…) ===
def df_to_download_bytes(df: pd.DataFrame, sheet_name: str = 'Sheet1'):
    buf = BytesIO()
    try:
        with pd.ExcelWriter(buf, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name=sheet_name)
        buf.seek(0)
        return buf.getvalue(), 'xlsxwriter'
    except Exception as e1:
        try:
            buf = BytesIO()
            with pd.ExcelWriter(buf, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name=sheet_name)
            buf.seek(0)
            return buf.getvalue(), 'openpyxl'
        except Exception as e2:
            st.info(f"ì—‘ì…€ ì—”ì§„ ì‚¬ìš© ì‹¤íŒ¨ë¡œ CSV(zip)ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. ì›ì¸: {type(e1).__name__} / {type(e2).__name__}")
            zbuf = BytesIO()
            with zipfile.ZipFile(zbuf, 'w', zipfile.ZIP_DEFLATED) as zf:
                zf.writestr('data.csv', df.to_csv(index=False, encoding='utf-8-sig'))
            zbuf.seek(0)
            return zbuf.getvalue(), 'zip'

# === 7) ì‚¬ì´ë“œë°”Â·ì•ˆë‚´ ë°°ë„ˆ(í™˜ê²½ ì˜ì¡´ ëŒ€ì•ˆ) ===
def ensure_sidebar_and_captions():
    try:
        with st.sidebar:
            st.markdown("### âš™ï¸ ë¶„ì„ ì˜µì…˜")
            if 'error_threshold_min' not in st.session_state:
                st.session_state['error_threshold_min'] = DEFAULT_ERROR_THRESHOLD_MIN
            new_v = st.number_input(
                "ì‘ì—…ì‹œê°„ ì˜¤ë¥˜ ì„ê³„ê°’(ë¶„)",
                min_value=60, max_value=1440,
                value=int(st.session_state.get('error_threshold_min', DEFAULT_ERROR_THRESHOLD_MIN)), step=10,
                help="í’ˆì§ˆ ì ê²€(ì˜¤ë¥˜ íƒì§€)ìš© ì„ê³„ê°’ì…ë‹ˆë‹¤. ë¶„ì„ ì œì™¸ ê¸°ì¤€(10ì‹œê°„ ì»·)ê³¼ ë‹¤ë¦…ë‹ˆë‹¤."
            )
            st.session_state['error_threshold_min'] = int(new_v)
        st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ì¤€/ì„ê³„ê°’ ì¡°ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    except Exception:
        pass

# (ìë™ í˜¸ì¶œ ì‹œ ë©”ì¸ ì½”ë“œ ìœ„ì— ìˆì–´ë„ ë¬´í•´)
# (íŒ¨ì¹˜) ensure_sidebar_and_captions()ëŠ” set_page_config ì´í›„ì— í˜¸ì¶œë©ë‹ˆë‹¤.
# ê³µí†µ ë¬¸ìì—´ ì²˜ë¦¬ ìœ í‹¸(ì¤‘ë³µ ì œê±°ìš©)
def sstr(s: pd.Series) -> pd.Series:
    return s.astype(str).str.strip()

# âš™ï¸ Page config â€” MUST be the first Streamlit command
st.set_page_config(page_title="ì—…ë¬´ì¼ì§€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="wide", initial_sidebar_state="collapsed")


# (íŒ¨ì¹˜) set_page_config ì§í›„ì— ì‚¬ì´ë“œë°”/ìº¡ì…˜ í‘œì‹œ
try:
    ensure_sidebar_and_captions()
except Exception:
    pass
# âœ… Query params helpers (replace deprecated experimental_* with st.query_params)
#    - ì½ê¸°: st.query_params.get("key", default)
#    - ì“°ê¸°: st.query_params["key"] = "value"  (ë¬¸ìì—´)

def ensure_sidebar_open_once():
    """URL ì¿¼ë¦¬íŒŒë¼ë¯¸í„° sb=1ì„ ì„¤ì •í•˜ê³  1íšŒ reruní•˜ì—¬, ì´í›„ ìƒˆë¡œê³ ì¹¨ ì‹œ 'ì´ˆê¸° í¼ì¹¨'ì´ ì ìš©ë˜ë„ë¡ ìœ ë„.
    êµ¬ë²„ì „ Streamlitì—ì„œëŠ” set_page_configë¥¼ ë‘ ë²ˆ í˜¸ì¶œí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì•ˆì „í•˜ê²Œ íŒŒë¼ë¯¸í„°ë§Œ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    try:
        if st.query_params.get("sb", "0") != "1":
            st.query_params["sb"] = "1"
            st.rerun()
    except Exception:
        pass

# (ì„ íƒ) ìµœì‹  ë²„ì „ì—ì„œë§Œ ë™ì‘: ì¿¼ë¦¬íŒŒë¼ë¯¸í„°ë¡œ í™•ì¥ ìš”ì²­ ì‹œ ì‹œë„
try:
    if st.query_params.get("sb", "0") == "1":
        pass  # auto-open sidebar handled via ensure_sidebar_open_once + rerun
except Exception:
    pass

# âœ… ë¡œê³  base64 ì¸ì½”ë”©í•´ì„œ ì„¸ì…˜ì— ì €ì¥
@st.cache_data
def load_logo_base64(path='ë¡œê³ .jpg'):
    with open(path, "rb") as image_file:
        encoded = base64.b64encode(image_file.read()).decode()
        return encoded

if 'logo_base64' not in st.session_state:
    try:
        st.session_state['logo_base64'] = load_logo_base64("ë¡œê³ .jpg")
    except FileNotFoundError:
        st.session_state['logo_base64'] = ""


def split_workers(worker_string):
    worker_string = re.sub(r'[.,/;Â·\sï¼]+', ',', str(worker_string))  # ì‰¼í‘œ, ë§ˆì¹¨í‘œ, ê³µë°± â†’ ì‰¼í‘œ
    worker_string = re.sub(r'(?<=[ê°€-í£]{2})(?=[ê°€-í£]{2})', ',', worker_string)  # ë¶™ì—¬ì“°ê¸°ëœ í•œê¸€ ì´ë¦„ ë¶„ë¦¬
    return [name.strip() for name in worker_string.split(',') if name.strip()]


def process_data(uploaded_file, dayfirst=False):
    # CSV ì½ê¸°: ë‚ ì§œ ì»¬ëŸ¼ ì¦‰ì‹œ íŒŒì‹± + ì¸ì½”ë”© í´ë°±
    try:
        df_original = pd.read_csv(
            uploaded_file,
            parse_dates=['ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ'],
            dayfirst=dayfirst
        )
    except UnicodeDecodeError:
        uploaded_file.seek(0)
        df_original = pd.read_csv(
            uploaded_file,
            parse_dates=['ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ'],
            dayfirst=dayfirst,
            encoding='cp949'
        )

    # ë°ì´í„° ê°€ê³µ
    df = df_original.copy()
    df['ì›ë³¸ì‘ì—…ì'] = df['ì‘ì—…ì']
    df['ì‘ì—…ì‹œê°„(ë¶„)'] = (df['ì¢…ë£Œì¼ì‹œ'] - df['ì‹œì‘ì¼ì‹œ']).dt.total_seconds() / 60
    # âœ… ë™ì¼ ì‘ì—…ì + ë™ì¼ ì‹œê°„ëŒ€ ì¤‘ë³µ ì œê±°
    df = df.drop_duplicates(subset=['ì‘ì—…ì', 'ì‹œì‘ì¼ì‹œ', 'ì¢…ë£Œì¼ì‹œ'])
    df['ì‘ì—…ìëª©ë¡'] = df['ì‘ì—…ì'].apply(split_workers)
    df['ì¡°êµ¬ì„±'] = df['ì‘ì—…ìëª©ë¡'].apply(lambda x: '2ì¸ 1ì¡°' if len(x) >= 2 else '1ì¸ 1ì¡°')
    df = df.explode('ì‘ì—…ìëª©ë¡')
    df['ì‘ì—…ì'] = df['ì‘ì—…ìëª©ë¡'].astype(str).str.strip()
    df.drop(columns=['ì‘ì—…ìëª©ë¡'], inplace=True)

    # ì›” ë‚´ ë‹¨ìˆœ ì£¼ì°¨(ì •ì±…ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
    df['ì£¼ì°¨'] = df['ì‹œì‘ì¼ì‹œ'].apply(lambda x: f"{x.month}ì›”{x.day // 7 + 1}ì£¼")
    # ISO ì£¼ì°¨ë„ í•¨ê»˜ ìƒì„±(ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒ ì ìš©)
    df['ISOì£¼ì°¨'] = df['ì‹œì‘ì¼ì‹œ'].dt.strftime('%G-W%V')

    # ë‚ ì§œ íƒ€ì… í†µì¼: datetime64[ns]ë¡œ ë³´ì •í•´ ë³‘í•© ì˜¤ë¥˜ ë°©ì§€
    df['ì‘ì—…ì¼'] = df['ì‹œì‘ì¼ì‹œ'].dt.normalize()
    return df, df_original


def convert_df_to_csv(df):
    return df.to_csv(index=False).encode('utf-8-sig')

# ğŸ“¦ ìºì‹œ ì²˜ë¦¬: íŒŒì¼ ë°”ì´íŠ¸ + dayfirstë¥¼ í‚¤ë¡œ ì‚¬ìš©
@st.cache_data
def cached_process(file_bytes, dayfirst):
    buf = BytesIO(file_bytes)
    buf.seek(0)
    return process_data(buf, dayfirst)

# ğŸ“¦ ë©€í‹°ì‹œíŠ¸ ì—‘ì…€ ë³€í™˜
def to_excel(sheets: dict):
    """ë©€í‹°ì‹œíŠ¸ XLSX ìƒì„±. ì—”ì§„ ë¯¸ì„¤ì¹˜ ì‹œ ZIP(CSV ë¬¶ìŒ)ìœ¼ë¡œ í´ë°±.
    Returns: (bytes, fmt) where fmt in {"xlsx", "zip"}
    """
    # 1) ê°€ëŠ¥í•œ ì—‘ì…€ ì—”ì§„ ì‹œë„(openpyxl â†’ xlsxwriter)
    for engine in ("openpyxl", "xlsxwriter"):
        try:
            output = BytesIO()
            with pd.ExcelWriter(output, engine=engine) as writer:
                for name, d in sheets.items():
                    if hasattr(d, 'data'):
                        d = d.data
                    d.to_excel(writer, index=False, sheet_name=name[:31])
            return output.getvalue(), "xlsx"
        except ModuleNotFoundError:
            continue
        except Exception:
            # ë‹¤ë¥¸ ì˜ˆì™¸ëŠ” ë‹¤ìŒ ì˜µì…˜ìœ¼ë¡œ í´ë°±
            continue

    # 2) ì—‘ì…€ ì—”ì§„ì´ ì—†ìœ¼ë©´ ZIPìœ¼ë¡œ CSV ë¬¶ìŒ ì œê³µ
    zbuf = BytesIO()
    with zipfile.ZipFile(zbuf, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:
        for name, d in sheets.items():
            if hasattr(d, 'data'):
                d = d.data
            safe = f"{name[:31]}.csv"
            zf.writestr(safe, d.to_csv(index=False, encoding='utf-8-sig'))
    return zbuf.getvalue(), "zip"




# í‘œì‹œìš©: ì‹œì‘/ì¢…ë£Œ ì¼ì‹œì—ì„œ ì´ˆ ë‹¨ìœ„ ì œê±°(í•´ë‹¹ ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ)
def format_dt_display(df: pd.DataFrame) -> pd.DataFrame:
    disp = df.copy()
    for col in ("ì‹œì‘ì¼ì‹œ", "ì¢…ë£Œì¼ì‹œ"):
        if col in disp.columns:
            if pd.api.types.is_datetime64_any_dtype(disp[col]):
                disp[col] = disp[col].dt.strftime('%Y-%m-%d %H:%M')
            else:
                disp[col] = pd.to_datetime(disp[col], errors='coerce').dt.strftime('%Y-%m-%d %H:%M')
    return disp


def main():
    col1, col2 = st.columns([8, 1])
    with col1:
        st.markdown("""
<h1 style='font-size: 50px;'>ğŸ“Š  <span style='color:#d32f2f;'>MOS</span>tagram ë¶„ì„ ëŒ€ì‹œë³´ë“œ</h1>
""", unsafe_allow_html=True)
    with col2:
        logo_base64 = st.session_state.get('logo_base64', "")
        if logo_base64:
            st.markdown(f"""
            <div style='display: flex; justify-content: flex-end;'>
                <img src='data:image/jpeg;base64,{logo_base64}' width='180'>
            </div>
            """, unsafe_allow_html=True)

    st.markdown("""
<p style='font-size: 25px;'>ì—…ë¬´ì¼ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³ , íŒ€ê³¼ íŒ€ì›ë³„ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.</p>
""", unsafe_allow_html=True)

    uploaded_file = st.file_uploader("ğŸ“ work_report.csv íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])
    st.markdown("""
<div style='padding: 12px; background-color: #f0f8ff; border-left: 5px solid #0072C6; font-weight: bold; font-size: 16px;'>
ğŸ“¤ MOStagramì—ì„œ ì—…ë¬´ì¼ì§€ ë°ì´í„° íŒŒì¼ì„ <b>ë‹¤ìš´ë¡œë“œí•œ í›„</b>, í•´ë‹¹ íŒŒì¼ì„ 
<b><span style='color:red;'>Browse files</span></b> ë²„íŠ¼ì„ í†µí•´ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ 
<b>ë¶„ì„ ëŒ€ì‹œë³´ë“œ</b>ê°€ ë Œë”ë§ë©ë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True)

    # ğŸ§° ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© (íŒŒì¼ì´ ì—†ì„ ë•Œ UI ì²´í—˜ìš©)
    with st.expander("ğŸ§° ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© (íŒŒì¼ì´ ì—†ì„ ë•Œ UI ì²´í—˜ìš©)", expanded=False):
        if st.button("ìƒ˜í”Œ CSV ë¶ˆëŸ¬ì˜¤ê¸°"):
            sample = pd.DataFrame({
                'íŒ€': ['A','A','B','B'],
                'ì‘ì—…ì': ['í™ê¸¸ë™','ê¹€ì² ìˆ˜','ì´ì˜í¬','ë°•ì˜ìˆ˜'],
                'ì‹œì‘ì¼ì‹œ': pd.to_datetime(['2025-08-10 09:00','2025-08-10 10:00','2025-08-11 09:30','2025-08-11 11:00']),
                'ì¢…ë£Œì¼ì‹œ': pd.to_datetime(['2025-08-10 12:00','2025-08-10 15:00','2025-08-11 13:00','2025-08-11 14:30']),
                'ì—…ë¬´ì¢…ë¥˜': ['ë¬´ì„ ','ë¬´ì„ ','ìœ ì„ ','ë¬´ì„ '],
                'êµ¬ë¶„': ['ì¥ì• /ì•ŒëŒ(AS)','ì‚¬ë¬´ì—…ë¬´','ì¥ì• /ì•ŒëŒ(AS)','ì¥ì• /ì•ŒëŒ(AS)'],
                'ì¥ë¹„ID': ['E1','E2','E3','E3'],
                'ì¥ë¹„ëª…': ['êµ­ì†Œ1','êµ­ì†Œ2','êµ­ì†Œ3','êµ­ì†Œ3'],
            })
            st.session_state['sample_df'] = sample
            st.success("ìƒ˜í”Œ ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë¶„ì„ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
            # â–¶ ìƒ˜í”Œ ë¡œë“œ ì‹œì—ë„ ì‚¬ì´ë“œë°” ìë™ ì—´ë¦¼ (1íšŒë§Œ rerun)
            ensure_sidebar_open_once()
    # ğŸ“… íŒŒì‹± ì„¤ì •(ì—…ë¡œë“œ ì „ì— ì„ íƒ ê°€ëŠ¥)
    with st.sidebar:
        st.subheader("ğŸ“… íŒŒì‹± ì„¤ì •")
        dayfirst_opt = st.checkbox("ë‚ ì§œê°€ 'ì¼/ì›”/ì—°' í˜•ì‹(dayfirst)", value=False)

    if uploaded_file or st.session_state.get('sample_df') is not None:
        # ì—…ë¡œë“œ/ìƒ˜í”Œ ë¶„ê¸°
        if uploaded_file:
            # âœ… ì»¬ëŸ¼ ì¡´ì¬ ê²€ì¦(ì—…ë¡œë“œ ì§í›„) â€” ì¸ì½”ë”© ëŒ€ë¹„
            try:
                cols_df = pd.read_csv(uploaded_file, nrows=0)
            except UnicodeDecodeError:
                uploaded_file.seek(0)
                cols_df = pd.read_csv(uploaded_file, nrows=0, encoding='cp949')
            except Exception as e:
                st.error(f"CSV íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.stop()
            required_cols = ['íŒ€','ì‘ì—…ì','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ','ì—…ë¬´ì¢…ë¥˜','êµ¬ë¶„','ì¥ë¹„ID','ì¥ë¹„ëª…']
            missing = [c for c in required_cols if c not in cols_df.columns]
            if missing:
                st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing} â€” CSV ì»¬ëŸ¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.stop()
            uploaded_file.seek(0)

            # âœ… ë°ì´í„° ê°€ê³µ
            # â–¶ ì—…ë¡œë“œ ì§í›„ ì‚¬ì´ë“œë°” ìë™ ì—´ë¦¼ (1íšŒë§Œ rerun)
            ensure_sidebar_open_once()
            file_bytes = uploaded_file.getvalue()
            with st.spinner("ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
                df, _ = cached_process(file_bytes, dayfirst_opt)
        else:
            # ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬
            sample_df = st.session_state.get('sample_df').copy()
            _buf = StringIO()
            sample_df.to_csv(_buf, index=False)
            _buf.seek(0)
            with st.spinner("ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
                df, _ = process_data(_buf, dayfirst_opt)

        # í‘œì¤€í™”(ë„ì–´ì“°ê¸°/ë™ì˜ì–´ ì •ë¦¬)
        df['êµ¬ë¶„'] = df['êµ¬ë¶„'].astype(str).str.strip().replace({
            'ì¥ì• /ì•ŒëŒ(AS)': 'ì¥ì• /ì•ŒëŒ',
            'ì‚¬ë¬´ì—…ë¬´ ': 'ì‚¬ë¬´ì—…ë¬´'
        })
        df['íŒ€'] = df['íŒ€'].astype(str).str.strip()

        st.success(f"ì—…ë¡œë“œ ì™„ë£Œ: {len(df):,}í–‰ ë¡œë“œ")

        
        with st.sidebar:
            st.header("ğŸ” ê²€ìƒ‰")
            min_date = df['ì‹œì‘ì¼ì‹œ'].min().date()
            max_date = df['ì¢…ë£Œì¼ì‹œ'].max().date()
            start_date, end_date = st.date_input("ì‘ì—… ê¸°ê°„ í•„í„°", [min_date, max_date], min_value=min_date, max_value=max_date)

            week_mode_options = ["ì›”ë‚´ì£¼ì°¨", "ISOì£¼ì°¨"]
            week_mode = st.radio("ì£¼ì°¨ ê¸°ì¤€", week_mode_options, index=0, horizontal=True)

            st.subheader("âš™ï¸ ì„¤ì •")
            daily_avg_threshold_hours = st.number_input("ì¼ë³„ í‰ê· ì‘ì—…ì‹œê°„ ê¸°ì¤€(ì‹œê°„)", min_value=0.0, max_value=24.0, value=6.2, step=0.1, format="%.1f")
            util_threshold = st.number_input("ê°€ë™ë¥  ê¸°ì¤€(%)", min_value=0, max_value=100, value=68, step=1)

            # ì‘ì—…ì‹œê°„ ì˜¤ë¥˜ ì„ê³„ê°’ (8ì‹œê°„ ê¸°ë³¸)
            error_threshold_h = st.number_input("ì‘ì—…ì‹œê°„ ì˜¤ë¥˜ ì„ê³„ê°’(ì‹œê°„)", min_value=1.0, max_value=24.0, value=8.0, step=0.5, format="%.1f")
            error_threshold_min = int(error_threshold_h * 60)
            st.caption(f"í˜„ì¬ ì„ê³„ê°’: {error_threshold_h:.1f}ì‹œê°„ = {error_threshold_min}ë¶„")

            with st.expander("ğŸ” ì¤‘ë³µ ì¶œë™ ê¸°ì¤€", expanded=False):
                dup_threshold = st.slider("ìµœì†Œ ì¶œë™ íšŸìˆ˜(ê±´)", 2, 10, 3, 1)
                recent_days = st.slider("ìµœê·¼ Nì¼ë§Œ ë³´ê¸°", 0, 90, 0, 1)

            # â­ í•„í„° ì¦ê²¨ì°¾ê¸° (ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°)
            # í•„í„° ì´ˆê¸°í™”
            if st.button("í•„í„° ì´ˆê¸°í™”"):
                st.rerun()

        df = df[(df['ì‹œì‘ì¼ì‹œ'].dt.date >= start_date) & (df['ì¢…ë£Œì¼ì‹œ'].dt.date <= end_date)]

        # ì£¼ì°¨ í‘œê¸° ê¸°ì¤€ ì„ íƒ ì ìš©
        if week_mode == "ì›”ë‚´ì£¼ì°¨":
            df['ì£¼ì°¨_í‘œì‹œ'] = df['ì£¼ì°¨']
        else:
            df['ì£¼ì°¨_í‘œì‹œ'] = df['ISOì£¼ì°¨']

        if 'íŒ€' in df.columns:
            team_options = df['íŒ€'].dropna().unique().tolist()
            team_list = ["ì „ì²´"] + team_options
            selected_team = st.sidebar.selectbox("íŒ€ ì„ íƒ", team_list, index=0)
            if selected_team != "ì „ì²´":
                df = df[df['íŒ€'] == selected_team]

            member_options = (
                df['ì‘ì—…ì']
                  .astype(str).str.strip()
                  .replace({'nan': ''})
                  .replace('', np.nan)
                  .dropna()
                  .unique().tolist()
            )
            with st.sidebar.expander("**ì‘ì—…ì ì„ íƒ**", expanded=False):
                selected_members = st.multiselect(
                    "ì‘ì—…ì ëª©ë¡",
                    options=member_options,
                    default=member_options
                )
            df = df[df['ì‘ì—…ì'].isin(selected_members)]
            st.session_state['selected_team'] = selected_team
            st.session_state['selected_members'] = selected_members

        # ğŸ” í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•„í„° (ì¥ë¹„ëª…/ì‘ì—…ì)
        with st.sidebar.expander("ğŸ” í…ìŠ¤íŠ¸ ê²€ìƒ‰", expanded=False):
            q = st.text_input("ì¥ë¹„ëª…/ì‘ì—…ì ê²€ìƒ‰(ë¶€ë¶„ì¼ì¹˜)", value="")
        st.session_state['q_text'] = q
        if q:
            df = df[
                df['ì¥ë¹„ëª…'].astype(str).str.contains(q, case=False, regex=False, na=False) |
                df['ì‘ì—…ì'].astype(str).str.contains(q, case=False, regex=False, na=False)
            ]
        # í•„í„° ìš”ì•½
        total_members = len(member_options) if 'member_options' in locals() else 0
        sel_members = st.session_state.get('selected_members', [])
        sel_text = "ì „ì²´" if (not sel_members or (total_members and len(sel_members) == total_members)) else f"{len(sel_members)}/{total_members}ëª…"
        st.info(f"ê¸°ê°„: {start_date}~{end_date} | íŒ€: {st.session_state.get('selected_team','ì „ì²´')} | ì‘ì—…ì ì„ íƒ: {sel_text} | ê²€ìƒ‰ì–´: {q or '-'} | ì„ê³„ê°’: {error_threshold_h:.1f}ì‹œê°„({error_threshold_min}ë¶„)")

        # íŒŒì¼ëª… íƒœê·¸(íŒ€/ê¸°ê°„) ê³µí†µ ì •ì˜

        # ë°ì´í„° ì—†ìŒ ë°©ì§€
        if df.empty:
            st.warning("ì„ íƒí•œ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
            st.markdown("## âš ï¸ ì‘ì—…ì‹œê°„ ì˜¤ë¥˜")
            st.write("- 0ë¶„ ì‘ì—…ì‹œê°„: **0ê±´**")
            st.write(f"- íŒ€ ê²°ì¸¡: **0ê±´**")
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'í•„í„° ì´ˆê¸°í™”'ë¥¼ ëˆ„ë¥´ê±°ë‚˜ ê¸°ê°„/ê²€ìƒ‰ì–´ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
            st.stop()

        # âœ… ì‚¬ì´ë“œë°” í•˜ë‹¨ì— CSV ì €ì¥ ë²„íŠ¼
        st.sidebar.markdown("---")
        st.sidebar.markdown("### ğŸ’¾ ì „ì²´ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
        csv = convert_df_to_csv(df)
        st.sidebar.download_button(
            label="ğŸ“¥ CSV íŒŒì¼ ì €ì¥",
            data=csv,
            file_name="ì—…ë¬´ì¼ì§€_ë¶„ì„ê²°ê³¼.csv",
            mime="text/csv"
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # âœ… ì¤‘ë³µ ì¶œë™ í˜„í™©  (ë¨¼ì € í‘œì‹œ)
        st.markdown("## ğŸ” ì¤‘ë³µ ì¶œë™ í˜„í™©")

        # í•„í„°ë§ í•œ ë²ˆë§Œ ìˆ˜í–‰
        dup_equipment = df[
            (df['ì—…ë¬´ì¢…ë¥˜'] == 'ë¬´ì„ ') &
            (df['êµ¬ë¶„'] == 'ì¥ì• /ì•ŒëŒ') &
            df['ì¥ë¹„ID'].notna() & (sstr(df['ì¥ë¹„ID']) != '') &
            df['ì¥ë¹„ëª…'].notna() & (sstr(df['ì¥ë¹„ëª…']) != '') &
            (~sstr(df['ì¥ë¹„ëª…']).str.contains('ë¯¼ì›', regex=False)) &
            (~sstr(df['ì¥ë¹„ëª…']).str.contains('ì‚¬ë¬´', regex=False))
        ].copy()

        if 'recent_days' in locals() and recent_days > 0:
            cutoff = pd.to_datetime(end_date) - pd.Timedelta(days=recent_days)
            dup_equipment = dup_equipment[dup_equipment['ì‹œì‘ì¼ì‹œ'] >= cutoff]

        # ë°©ë¬¸(ì‹¤ì œ ì¶œë™) ë‹¨ìœ„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        # - ì¤‘ë³µê±´ìˆ˜: íŒ€/ì¥ë¹„ëª…/ì¥ë¹„ID/ì‹œì‘ì¼ì‹œ/ì¢…ë£Œì¼ì‹œ(=1íšŒ ë°©ë¬¸) ìœ ë‹ˆí¬ ê°œìˆ˜
        # - ì‘ì—…ì ì¶œë™íšŸìˆ˜: ë°©ë¬¸-ì‘ì—…ì ìœ ë‹ˆí¬ ì¡°í•© ìˆ˜
        visit_keys = ['íŒ€','ì¥ë¹„ëª…','ì¥ë¹„ID','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']
        visits = dup_equipment[visit_keys].drop_duplicates()

        # ì¥ë¹„ë³„ ì¤‘ë³µê±´ìˆ˜ = ë°©ë¬¸ìˆ˜
        count_tbl = (
            visits
            .groupby(['íŒ€','ì¥ë¹„ëª…','ì¥ë¹„ID'], dropna=False)
            .size()
            .reset_index(name='ì¤‘ë³µê±´ìˆ˜')
        )

        # ì‘ì—…ìë³„ ì¶œë™ íšŸìˆ˜ = ë°©ë¬¸-ì‘ì—…ì ìœ ë‹ˆí¬ ì¡°í•© ìˆ˜
        worker_cnt = (
            dup_equipment[['íŒ€','ì¥ë¹„ëª…','ì¥ë¹„ID','ì‘ì—…ì','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']]
            .drop_duplicates()
            .groupby(['íŒ€','ì¥ë¹„ëª…','ì¥ë¹„ID','ì‘ì—…ì'])
            .size()
            .reset_index(name='ë°©ë¬¸ìˆ˜')
        )
        worker_list = (
            worker_cnt
            .assign(í‘œì‹œ=lambda d: sstr(d['ì‘ì—…ì']) + '(' + d['ë°©ë¬¸ìˆ˜'].astype(int).astype(str) + ')')
            .groupby(['íŒ€','ì¥ë¹„ëª…','ì¥ë¹„ID'])['í‘œì‹œ']
            .apply(lambda s: ', '.join(s))
            .reset_index(name='ì‘ì—…ì(ì¶œë™ íšŸìˆ˜)')
        )

        combined = (
            count_tbl.merge(worker_list, on=['íŒ€','ì¥ë¹„ëª…','ì¥ë¹„ID'], how='left')
            .query('ì¤‘ë³µê±´ìˆ˜ >= @dup_threshold')
            .sort_values('ì¤‘ë³µê±´ìˆ˜', ascending=False)
            .reset_index(drop=True)
        )

        dup_display = combined.rename(columns={'íŒ€': 'ìš´ìš©íŒ€'})

        # ğŸ“ˆ ë©”íŠ¸ë¦­(ì¤‘ë³µ ì¶œë™)
        _dup_cnt = int(len(dup_display))
        _dup_max = int(dup_display['ì¤‘ë³µê±´ìˆ˜'].max()) if not dup_display.empty else 0
        _dup_avg = float(dup_display['ì¤‘ë³µê±´ìˆ˜'].mean()) if not dup_display.empty else 0.0
        m1, m2, m3 = st.columns(3)
        m1.metric("ì¤‘ë³µ ì¥ë¹„ ìˆ˜", f"{_dup_cnt}")
        m2.metric("ìµœëŒ€ ì¤‘ë³µ íšŸìˆ˜", _dup_max)
        m3.metric("í‰ê·  ì¤‘ë³µ íšŸìˆ˜", f"{_dup_avg:.1f}")

        # íŒŒì¼ëª… íƒœê·¸(íŒ€/ê¸°ê°„)
        date_tag = f"{start_date}_{end_date}"
        team_tag = "ì „ì²´" if st.session_state.get('selected_team') in [None, "ì „ì²´"] else st.session_state['selected_team']

        # ğŸ” ë“œë¦´ë‹¤ìš´: ì¥ë¹„ëª… ì„ íƒ(ë©”íŠ¸ë¦­ ë°”ë¡œ ì•„ë˜)
        if not dup_display.empty:
            _names = sstr(dup_display['ì¥ë¹„ëª…']).unique().tolist()
            _sel = st.selectbox("ğŸ” ì¥ë¹„ëª… ì„ íƒ(ë“œë¦´ë‹¤ìš´)", ["ì„ íƒ ì•ˆí•¨"] + _names, index=0)
            if _sel != "ì„ íƒ ì•ˆí•¨":
                det = df[sstr(df['ì¥ë¹„ëª…']) == str(_sel)].copy()
                det['ì‘ì—…ì'] = det['ì‘ì—…ì'].astype(str).str.strip()

                visit_keys = ['íŒ€','ì¥ë¹„ëª…','ì¥ë¹„ID','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ','êµ¬ë¶„']
                workers_join = (
                    det.groupby(visit_keys)['ì‘ì—…ì']
                      .apply(lambda s: ', '.join(sorted(set(s))))
                      .reset_index()
                )

                # â–¶ ì‘ì—…ë‚´ìš©(í•´ì‹œíƒœê·¸) ë³‘í•©: í›„ë³´ ì»¬ëŸ¼ ìë™ íƒìƒ‰ í›„ 'ì‘ì—…ë‚´ìš©'ìœ¼ë¡œ í‘œì‹œ
                _norm = lambda x: ''.join(str(x).split()).lower()
                _colmap = {_norm(c): c for c in det.columns}
                _cands = ['ì‘ì—…ë‚´ìš©','í•´ì‹œíƒœê·¸','í•´ì‹œíƒœê·¸(ì‘ì—…ë‚´ìš©)','hashtags','hashtag','ì—…ë¬´ë‚´ìš©','ë‚´ìš©']
                _src = None
                for _c in _cands:
                    if _norm(_c) in _colmap:
                        _src = _colmap[_norm(_c)]
                        break
                if _src is not None:
                    _content = (
                        det.groupby(visit_keys)[_src]
                          .apply(lambda s: ', '.join(sorted(set([str(x).strip() for x in s if str(x).strip() and str(x).strip().lower()!='nan']))))
                          .reset_index()
                          .rename(columns={_src:'ì‘ì—…ë‚´ìš©'})
                    )
                else:
                    _content = (
                        det.groupby(visit_keys).size().reset_index(name='__tmp').drop(columns='__tmp').assign(ì‘ì—…ë‚´ìš©='-')
                    )

                # ë™ì¼ ë°©ë¬¸ì˜ ì‘ì—…ì‹œê°„(ë¶„)ì€ ë™ì¼í•˜ë¯€ë¡œ ì²« ê°’ ì‚¬ìš©
                dur = det.groupby(visit_keys)['ì‘ì—…ì‹œê°„(ë¶„)'].first().reset_index()

                det_view = workers_join.merge(_content, on=visit_keys, how='left').merge(dur, on=visit_keys, how='left')
                det_view = det_view[['íŒ€','ì‘ì—…ì','êµ¬ë¶„','ì‘ì—…ë‚´ìš©','ì¥ë¹„ëª…','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ','ì‘ì—…ì‹œê°„(ë¶„)']].sort_values('ì‹œì‘ì¼ì‹œ')

                st.dataframe(format_dt_display(det_view), use_container_width=True)
                st.download_button(
                    "â¬‡ï¸ ì„ íƒ ì¥ë¹„ ìƒì„¸ CSV",
                    data=convert_df_to_csv(det_view),
                    file_name=f"ì¥ë¹„ìƒì„¸_{_sel}_{team_tag}_{date_tag}.csv",
                    mime="text/csv"
                )

        st.dataframe(
            format_dt_display(dup_display),
            use_container_width=True,
            column_config={'ì¤‘ë³µê±´ìˆ˜': st.column_config.NumberColumn(format="%d")}
        )
        st.download_button(
            "â¬‡ï¸ ì¤‘ë³µ ì¶œë™ í˜„í™© CSV",
            data=convert_df_to_csv(dup_display),
            file_name=f"ì¤‘ë³µì¶œë™í˜„í™©_{team_tag}_{date_tag}.csv",
            mime="text/csv"
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ‘¤ ê°œì¸ë³„ ëˆ„ë½ í˜„í™© â€” ì¤‘ë³µ ë¶„ë¦¬ ì œê±°(ì´ë¯¸ process_dataì—ì„œ explodeë¨)
        st.markdown("## ğŸ“‹ ê°œì¸ë³„ ëˆ„ë½ í˜„í™©")
        
        # ëª¨ë“  ì‘ì—…ì ëª©ë¡ & ì˜ì—…ì¼ ë‚ ì§œ ìƒì„±
        workers = df[['íŒ€','ì‘ì—…ì']].dropna().drop_duplicates()
        date_range = pd.date_range(start=df['ì‘ì—…ì¼'].min(), end=df['ì‘ì—…ì¼'].max(), freq='B')
        all_worker_days = pd.MultiIndex.from_product([workers['ì‘ì—…ì'], date_range], names=['ì‘ì—…ì','ì‘ì—…ì¼'])
        all_worker_days = pd.DataFrame(index=all_worker_days).reset_index().merge(workers, on='ì‘ì—…ì', how='left')

        # ì¼ìë³„ ì‘ì„±ì—¬ë¶€(ì—¬ëŸ¬ ê±´â†’1)
        actual_logs = df.groupby(['íŒ€','ì‘ì—…ì','ì‘ì—…ì¼']).size().rename('ì‘ì„±ì—¬ë¶€').reset_index()
        actual_logs['ì‘ì„±ì—¬ë¶€'] = 1
        log_df = all_worker_days.merge(actual_logs, on=['íŒ€','ì‘ì—…ì','ì‘ì—…ì¼'], how='left').fillna({'ì‘ì„±ì—¬ë¶€':0})

        # ğŸ“ˆ ë©”íŠ¸ë¦­(ëˆ„ë½ í˜„í™©)
        _ps_all = log_df.groupby(['íŒ€','ì‘ì—…ì'])['ì‘ì„±ì—¬ë¶€'].agg(['mean','count']).reset_index()
        _ps_all['ëˆ„ë½ì¼ìˆ˜'] = (1 - _ps_all['mean']) * _ps_all['count']
        n1, n2, n3 = st.columns(3)
        n1.metric("ëˆ„ë½ ëŒ€ìƒ ì¸ì›", int((_ps_all['mean'] < 1.0).sum()))
        n2.metric("ì´ ëˆ„ë½ ì¼ìˆ˜", int(_ps_all['ëˆ„ë½ì¼ìˆ˜'].sum()))
        n3.metric("í‰ê·  ëˆ„ë½ë¥ (ì „ì²´)", f"{int((1 - log_df['ì‘ì„±ì—¬ë¶€'].mean()) * 100)}%")

        # âœ” ê°œì¸ë³„ ëˆ„ë½ í˜„í™© â€” í‘œ í˜•ì‹(ì¤‘ë³µ ì¶œë™ í˜„í™©ê³¼ ë™ì¼ ìŠ¤íƒ€ì¼)
        personal_summary = _ps_all[_ps_all['mean'] < 1.0].copy()
        personal_summary['ëˆ„ë½ë¥ (%)'] = (1 - personal_summary['mean']) * 100
        personal_summary['ëˆ„ë½ì¼ìˆ˜'] = personal_summary['ëˆ„ë½ì¼ìˆ˜'].astype(int)
        personal_summary['ëˆ„ë½ë¥ (%)'] = personal_summary['ëˆ„ë½ë¥ (%)'].astype(int)
        table_df = (
            personal_summary[['íŒ€','ì‘ì—…ì','ëˆ„ë½ì¼ìˆ˜','ëˆ„ë½ë¥ (%)']]
            .sort_values(by=['ëˆ„ë½ì¼ìˆ˜','ëˆ„ë½ë¥ (%)'], ascending=[False, False])
            .rename(columns={'íŒ€':'ìš´ìš©íŒ€'})
            .reset_index(drop=True)
        )
        st.dataframe(table_df, use_container_width=True, column_config={'ëˆ„ë½ì¼ìˆ˜': st.column_config.NumberColumn(format="%d"), 'ëˆ„ë½ë¥ (%)': st.column_config.NumberColumn(format="%d%%")})
        st.download_button("â¬‡ï¸ ê°œì¸ë³„ ëˆ„ë½ í˜„í™© CSV", data=convert_df_to_csv(table_df), file_name=f"ê°œì¸ë³„ëˆ„ë½í˜„í™©_{team_tag}_{date_tag}.csv", mime="text/csv")

        # â— ë°ì´í„° í’ˆì§ˆ ì ê²€ (í•­ìƒ í‘œì‹œ)
        st.markdown("## âš ï¸ ì‘ì—…ì‹œê°„ ì˜¤ë¥˜")
        zero_cnt = int((df['ì‘ì—…ì‹œê°„(ë¶„)'] == 0).sum())
        null_team = int(df['íŒ€'].isna().sum())
        # ì˜¤ë¥˜ ìš”ì•½ ë©”íŠ¸ë¦­ ì¹´ë“œ
        long_cnt = int((df['ì‘ì—…ì‹œê°„(ë¶„)'] >= error_threshold_min).sum())
        kz1, kz2, kz3 = st.columns(3)
        kz1.metric("0ë¶„ ì‘ì—…", zero_cnt)
        kz2.metric("ì„ê³„ê°’ ì´ˆê³¼", long_cnt)
        kz3.metric("íŒ€ ê²°ì¸¡", null_team)

        # â¬‡ï¸ ë¬¸ì œ í–‰ ë‹¤ìš´ë¡œë“œ (ìŒìˆ˜/0ë¶„/ë‚ ì§œ ì˜¤ë¥˜ê±´)
        zero_rows = df[df['ì‘ì—…ì‹œê°„(ë¶„)'] == 0][['íŒ€','ì‘ì—…ì','êµ¬ë¶„','ì¥ë¹„ëª…','ì‘ì—…ì‹œê°„(ë¶„)','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']]
        long_rows = df[df['ì‘ì—…ì‹œê°„(ë¶„)'] >= error_threshold_min][['íŒ€','ì‘ì—…ì','êµ¬ë¶„','ì¥ë¹„ëª…','ì‘ì—…ì‹œê°„(ë¶„)','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']]
        c1, c2 = st.columns(2)
        with c1:
            st.download_button("â¬‡ï¸ 0ë¶„ ì‘ì—…ì‹œê°„ CSV", data=convert_df_to_csv(zero_rows), file_name=f"0ë¶„_ì‘ì—…ì‹œê°„_{team_tag}_{date_tag}.csv", mime="text/csv")
        with c2:
            st.download_button("â¬‡ï¸ ì‘ì—…ì‹œê°„ ì„ê³„ê°’ ì´ˆê³¼ CSV", data=convert_df_to_csv(long_rows), file_name=f"ì‘ì—…ì‹œê°„_ì„ê³„ê°’_ì´ˆê³¼_{team_tag}_{date_tag}.csv", mime="text/csv")
        # ğŸ“‹ ì‘ì—…ì‹œê°„ ì˜¤ë¥˜ ëª©ë¡
        tab1, tab2 = st.tabs(["ì„ê³„ê°’ ì´ˆê³¼ ëª©ë¡", "0ë¶„ ì‘ì—… ëª©ë¡"])
        with tab1:
            keep_sort = st.checkbox("ì‘ì—…ì‹œê°„ ë‚´ë¦¼ì°¨ìˆœ ê³ ì •", value=True)
            long_rows_display = long_rows.copy()
            if keep_sort:
                long_rows_display = long_rows_display.sort_values('ì‘ì—…ì‹œê°„(ë¶„)', ascending=False)
            st.dataframe(format_dt_display(long_rows_display), use_container_width=True, height=420)
        with tab2:
            recent_first = st.checkbox("ìµœê·¼ìˆœ ë³´ê¸°", value=False, key="zero_recent_first")
            zero_rows_display = zero_rows.copy()
            if recent_first:
                zero_rows_display = zero_rows_display.sort_values('ì‹œì‘ì¼ì‹œ', ascending=False)
            else:
                zero_rows_display = zero_rows_display.sort_values(['íŒ€','ì‘ì—…ì','ì‹œì‘ì¼ì‹œ'])
            st.dataframe(format_dt_display(zero_rows_display), use_container_width=True, height=420)

        # ğŸš¨ ì‹œê°„ ì´ìƒ íƒì§€ â€” ê²¹ì¹¨/ì—­ì „
        st.markdown("## ğŸš¨ ì‹œê°„ ì´ìƒ íƒì§€ â€” ê²¹ì¹¨/ì—­ì „")
        # ì‹œì‘ > ì¢…ë£Œ (ì—­ì „)
        rev_rows = df[df['ì‹œì‘ì¼ì‹œ'] > df['ì¢…ë£Œì¼ì‹œ']][['íŒ€','ì‘ì—…ì','êµ¬ë¶„','ì¥ë¹„ëª…','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ','ì‘ì—…ì‹œê°„(ë¶„)']].copy()
        rev_cnt = int(len(rev_rows))
        # ê°™ì€ ì‘ì—…ìì˜ ì‹œê°„ ê²¹ì¹¨
        _sorted = df.sort_values(['ì‘ì—…ì','ì‹œì‘ì¼ì‹œ'])
        _prev_end = _sorted.groupby('ì‘ì—…ì')['ì¢…ë£Œì¼ì‹œ'].shift()
        overlap_mask = _sorted['ì‹œì‘ì¼ì‹œ'] < _prev_end
        overlap_rows = _sorted[overlap_mask][['íŒ€','ì‘ì—…ì','êµ¬ë¶„','ì¥ë¹„ëª…','ì‹œì‘ì¼ì‹œ','ì¢…ë£Œì¼ì‹œ']].copy()
        overlap_rows = overlap_rows.assign(ì´ì „ì¢…ë£Œ=_prev_end[overlap_mask])
        overlap_cnt = int(len(overlap_rows))
        e1, e2 = st.columns(2)
        with e1:
            st.metric("ì—­ì „ ì‹œê°„(ì‹œì‘>ì¢…ë£Œ)", rev_cnt)
            if rev_cnt > 0:
                st.download_button("â¬‡ï¸ ì—­ì „ ì‹œê°„ CSV", data=convert_df_to_csv(format_dt_display(rev_rows)), file_name=f"ì—­ì „ì‹œê°„_{team_tag}_{date_tag}.csv", mime="text/csv")
        with e2:
            st.metric("ì‹œê°„ ê²¹ì¹¨ ê±´ìˆ˜", overlap_cnt)
            if overlap_cnt > 0:
                st.download_button("â¬‡ï¸ ì‹œê°„ ê²¹ì¹¨ CSV", data=convert_df_to_csv(format_dt_display(overlap_rows)), file_name=f"ì‹œê°„ê²¹ì¹¨_{team_tag}_{date_tag}.csv", mime="text/csv")

        tab_ov, tab_rev = st.tabs(["ì‹œê°„ ê²¹ì¹¨ ëª©ë¡", "ì—­ì „ ì‹œê°„ ëª©ë¡"])
        with tab_ov:
            ov_recent = st.checkbox("ìµœê·¼ìˆœ ë³´ê¸°", value=False, key="ov_recent_first")
            ov_view = overlap_rows.copy()
            if ov_recent:
                ov_view = ov_view.sort_values('ì‹œì‘ì¼ì‹œ', ascending=False)
            else:
                ov_view = ov_view.sort_values(['íŒ€','ì‘ì—…ì','ì‹œì‘ì¼ì‹œ'])
            st.dataframe(format_dt_display(ov_view), use_container_width=True, height=420)
        with tab_rev:
            rev_recent = st.checkbox("ìµœê·¼ìˆœ ë³´ê¸°", value=False, key="rev_recent_first2")
            rev_view = rev_rows.copy()
            if rev_recent:
                rev_view = rev_view.sort_values('ì‹œì‘ì¼ì‹œ', ascending=False)
            else:
                rev_view = rev_view.sort_values(['íŒ€','ì‘ì—…ì','ì‹œì‘ì¼ì‹œ'])
            st.dataframe(format_dt_display(rev_view), use_container_width=True, height=420)

        st.markdown("## ğŸ•’ êµ¬ë¶„ë³„ MTTR / ë°˜ë³µë„")

        _mttr_keys = ['íŒ€', 'ì›ë³¸ì‘ì—…ì', 'ì‹œì‘ì¼ì‹œ', 'ì¢…ë£Œì¼ì‹œ', 'êµ¬ë¶„', 'ì¥ë¹„ID']
        mttr_df = df.drop_duplicates(subset=_mttr_keys).copy()

        # ì¥ë¹„ID ì •ë¦¬ ë° ìŒìˆ˜ ì‘ì—…ì‹œê°„ ì œê±°
        mttr_df['ì¥ë¹„ID'] = mttr_df['ì¥ë¹„ID'].astype(str).str.strip()
        # ìŒìˆ˜ ì œì™¸ + 8ì‹œê°„(480ë¶„) ì´ˆê³¼ ê±´ ì œì™¸
        mttr_df = mttr_df[(mttr_df['ì‘ì—…ì‹œê°„(ë¶„)'] >= 0) & (mttr_df['ì‘ì—…ì‹œê°„(ë¶„)'] <= error_threshold_min)]

        # âœ… 'ì‚¬ë¬´ì—…ë¬´' ì œì™¸
        mttr_df = mttr_df[mttr_df['êµ¬ë¶„'].astype(str).str.strip() != 'ì‚¬ë¬´ì—…ë¬´']

        # âœ… 'ì´ë™ì—…ë¬´' ì œì™¸ (ìš”ì²­ ì„¹ì…˜ í•œì •)
        mttr_df = mttr_df[mttr_df['ì—…ë¬´ì¢…ë¥˜'].astype(str).str.strip() != 'ì´ë™ì—…ë¬´']

        if mttr_df.empty:
            st.info("ë¶„ì„ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (í•„í„° ì¡°ê±´ì„ í™•ì¸í•´ ì£¼ì„¸ìš”)")
        else:
            def _p90(x):
                try:
                    return float(np.percentile(x, 90))
                except Exception:
                    return float(np.nan)

            mttr_tbl = (
                mttr_df
                .groupby('êµ¬ë¶„', dropna=False)['ì‘ì—…ì‹œê°„(ë¶„)']
                .agg(ê±´ìˆ˜='count', MTTR_ë¶„='mean', ì¤‘ì•™ê°’_ë¶„='median', P90_ë¶„=_p90)
                .reset_index()
            )

            rep_src = (
                mttr_df
                .loc[mttr_df['ì¥ë¹„ID'].notna() & (mttr_df['ì¥ë¹„ID'] != '')]
                .groupby(['êµ¬ë¶„', 'ì¥ë¹„ID']).size().reset_index(name='ê±´ìˆ˜')
            )
            rep_sum = rep_src.groupby('êµ¬ë¶„')['ì¥ë¹„ID'].nunique().reset_index(name='ê³ ìœ ì¥ë¹„ìˆ˜')
            rep_cnt = rep_src[rep_src['ê±´ìˆ˜'] >= 2].groupby('êµ¬ë¶„')['ì¥ë¹„ID'].nunique().reset_index(name='ì¬ë°œì¥ë¹„ìˆ˜')
            rep_tbl = rep_sum.merge(rep_cnt, on='êµ¬ë¶„', how='left').fillna({'ì¬ë°œì¥ë¹„ìˆ˜': 0})
            rep_tbl['ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)'] = (
                (rep_tbl['ì¬ë°œì¥ë¹„ìˆ˜'] / rep_tbl['ê³ ìœ ì¥ë¹„ìˆ˜']).replace([np.inf, np.nan], 0) * 100
            )

            result = mttr_tbl.merge(rep_tbl, on='êµ¬ë¶„', how='left')

            # âœ… ì •ìˆ˜ í‘œê¸°(ë¶„/ê°œìˆ˜/ë¹„ìœ¨)
            result['MTTR(ë¶„)'] = result['MTTR_ë¶„'].round().astype('Int64')
            result['ì¤‘ì•™ê°’(ë¶„)'] = result['ì¤‘ì•™ê°’_ë¶„'].round().astype('Int64')
            result['P90(ë¶„)'] = result['P90_ë¶„'].round().astype('Int64')
            result['ê³ ìœ  ì—…ë¬´ ìˆ˜'] = result['ê³ ìœ ì¥ë¹„ìˆ˜'].astype('Int64')
            result['ì¤‘ë³µì—…ë¬´ ìˆ˜'] = result['ì¬ë°œì¥ë¹„ìˆ˜'].astype('Int64')
            result['ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)'] = result['ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)'].round().astype('Int64')

            display_cols = ['êµ¬ë¶„', 'ê±´ìˆ˜', 'MTTR(ë¶„)', 'ì¤‘ì•™ê°’(ë¶„)', 'P90(ë¶„)', 'ê³ ìœ  ì—…ë¬´ ìˆ˜', 'ì¤‘ë³µì—…ë¬´ ìˆ˜', 'ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)']
            result_display = (
                result[display_cols]
                .sort_values(['MTTR(ë¶„)'], ascending=[True])
                .reset_index(drop=True)
            )
            st.dataframe(result_display, use_container_width=True, column_config={'ê±´ìˆ˜': st.column_config.NumberColumn(format="%d"), 'MTTR(ë¶„)': st.column_config.NumberColumn(format="%d"), 'ì¤‘ì•™ê°’(ë¶„)': st.column_config.NumberColumn(format="%d"), 'P90(ë¶„)': st.column_config.NumberColumn(format="%d"), 'ê³ ìœ  ì—…ë¬´ ìˆ˜': st.column_config.NumberColumn(format="%d"), 'ì¤‘ë³µì—…ë¬´ ìˆ˜': st.column_config.NumberColumn(format="%d"), 'ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)': st.column_config.NumberColumn(format="%d%%")})
            st.download_button("â¬‡ï¸ MTTR/ë°˜ë³µë„ ê²°ê³¼ CSV", data=convert_df_to_csv(result_display), file_name=f"MTTR_ë°˜ë³µë„_ê²°ê³¼_{team_tag}_{date_tag}.csv", mime="text/csv")

            # ğŸ“Œ ìš”ì•½ KPI
            k1, k2, k3, k4 = st.columns(4)
            k1.metric("ì´ ë¡œê·¸ ìˆ˜", f"{len(df):,}")
            k2.metric("íŒ€ ìˆ˜", int(df['íŒ€'].nunique()))
            try:
                avg_mttr = int(pd.to_numeric(result_display['MTTR(ë¶„)'], errors='coerce').dropna().mean())
                p90_dup = int(pd.to_numeric(result_display['ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)'], errors='coerce').dropna().quantile(0.90))
            except Exception:
                avg_mttr, p90_dup = 0, 0
            k3.metric("í‰ê·  MTTR(ë¶„)", avg_mttr)
            k4.metric("ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨ P90(%)", f"{p90_dup}%")

            result_display['MTTR_label'] = result_display['MTTR(ë¶„)'].astype('Int64').astype(str)
            # âœ… ì¶• ìƒí•œì„ ë™ì ìœ¼ë¡œ ë§ì¶° ë‘ ê·¸ë˜í”„ ë§‰ëŒ€ ë†’ì´ê°€ ë¹„ìŠ·í•˜ê²Œ ë³´ì´ë„ë¡ ì¡°ì •
            try:
                _mttr_max = float(result_display['MTTR(ë¶„)'].max())
                _dup_max = float(result_display['ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)'].max())
            except Exception:
                _mttr_max, _dup_max = 0.0, 0.0
            _mttr_ylim = max(10.0, _mttr_max * 1.1) if _mttr_max > 0 else 10.0
            _dup_ylim = min(100.0, max(10.0, _dup_max * 1.1)) if _dup_max > 0 else 10.0

            # âœ… êµ¬ë¶„ë³„ ìƒ‰ìƒ ê³ ì • ë§¤í•‘ (ë‘ ê·¸ë˜í”„ ê³µí†µ)
            cats = sorted(result_display['êµ¬ë¶„'].astype(str).unique())
            palette = px.colors.qualitative.Set2
            color_map = {c: palette[i % len(palette)] for i, c in enumerate(cats)}

            fig_mttr = px.bar(
                result_display,
                x='êµ¬ë¶„',
                y='MTTR(ë¶„)',
                color='êµ¬ë¶„',
                color_discrete_map=color_map,
                title='êµ¬ë¶„ë³„ MTTR(ë¶„)',
                labels={'MTTR(ë¶„)': 'MTTR(ë¶„)', 'êµ¬ë¶„': 'êµ¬ë¶„'},
                text='MTTR_label',
                custom_data=['MTTR_label']
            )
            fig_mttr.update_traces(textposition='outside', hovertemplate='êµ¬ë¶„: %{x}<br>MTTR(ë¶„): %{customdata[0]}<extra></extra>')
            fig_mttr.update_layout(legend_title_text='êµ¬ë¶„', margin=dict(t=60, b=0), height=420, bargap=0.2, yaxis_range=[0, _mttr_ylim])
            # (2ì—´ ë°°ì¹˜ë¡œ ì´ë™)

            # â–¼ êµ¬ë¶„ë³„ ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%) ê·¸ë˜í”„ â€” MTTR ê·¸ë˜í”„ ë°”ë¡œ ì•„ë˜ ë™ì¼ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
            result_display['dup_label'] = result_display['ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)'].fillna(0).astype('Int64').astype(str)
            fig_dup_ratio = px.bar(
                result_display,
                x='êµ¬ë¶„',
                y='ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)',
                color='êµ¬ë¶„',
                color_discrete_map=color_map,
                title='êµ¬ë¶„ë³„ ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)',
                labels={'ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)': 'ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨(%)', 'êµ¬ë¶„': 'êµ¬ë¶„'},
                text='dup_label',
                custom_data=['dup_label']
            )
            fig_dup_ratio.update_traces(
                textposition='outside',
                hovertemplate='êµ¬ë¶„: %{x}<br>ì¤‘ë³µì—…ë¬´ ë¹„ìœ¨: %{customdata[0]}%<extra></extra>'
            )
            fig_dup_ratio.update_layout(legend_title_text='êµ¬ë¶„', yaxis_range=[0, _dup_ylim], margin=dict(t=60, b=0), height=420, bargap=0.2)

            # âœ… ì¹´í…Œê³ ë¦¬ ìˆœì„œ ê³ ì •(ë‘ ê·¸ë˜í”„ ì¼ê´€ì„±)
            order = result_display.sort_values('MTTR(ë¶„)')['êµ¬ë¶„'].astype(str).tolist()
            fig_mttr.update_layout(xaxis={'categoryorder': 'array', 'categoryarray': order})
            fig_dup_ratio.update_layout(xaxis={'categoryorder': 'array', 'categoryarray': order})

            cols_mttr = st.columns(2)
            with cols_mttr[0]:
                st.plotly_chart(fig_mttr, use_container_width=True)
            with cols_mttr[1]:
                st.plotly_chart(fig_dup_ratio, use_container_width=True)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ—“ï¸ ìš´ìš©íŒ€ ì¼ë³„ ì‘ì„±í˜„í™© (ì´ë™ì—…ë¬´ í¬í•¨)
        st.markdown("## ğŸ—“ï¸ ìš´ìš©íŒ€ ì¼ë³„ ì‘ì„± í˜„í™©")
        daily_count = df.groupby([df['ì‹œì‘ì¼ì‹œ'].dt.date, df['íŒ€']]).size().unstack(fill_value=0).astype(int)
        daily_count.loc['í•©ê³„'] = daily_count.sum()
        st.dataframe(daily_count, use_container_width=True)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ“‰ íŒ€ ì£¼ì°¨ë³„ ê°€ë™ë¥  (ì´ë™ì—…ë¬´ í¬í•¨)
        st.markdown("## ğŸ“‰ íŒ€ ì£¼ì°¨ë³„ ê°€ë™ë¥ ")

        # ì‘ì—…ì‹œê°„ ê¸°ë°˜(ì¸ì›=ì „ì²´, ëˆ„ë½ë¥ â‰¥50% ì œì™¸)
# (ìë™ì •ë¦¬) ì¤‘ë³µ ì •ì˜ ì œê±°:         BASIS_MIN_PER_DAY = 480  # ê¸°ì¤€ ê·¼ë¬´ì‹œê°„(ë¶„/ì¼)

        # 1) ë‹¨ì¼ ê¸°ë¡ ê¸°ì¤€ 'ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì‹œê°„' ì œì™¸ â€” í•œ ì‘ì—…ë‹¹ 10ì‹œê°„(600ë¶„) ì´ˆê³¼ ì œì™¸
# (ìë™ì •ë¦¬) ì¤‘ë³µ ì •ì˜ ì œê±°:         ABNORMAL_TASK_MIN = 600  # 10ì‹œê°„(ë¶„)
        util_df = df[(df['ì‘ì—…ì‹œê°„(ë¶„)'] >= 0) & (df['ì‘ì—…ì‹œê°„(ë¶„)'] < ABNORMAL_TASK_MIN)].copy()

        # ë°ì´í„° ì—†ìœ¼ë©´ ì•ˆë‚´ í›„ ì„¹ì…˜ ì¢…ë£Œ
        if util_df.empty:
            st.info("ê°€ë™ë¥ ì„ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë‹¨ì¼ ê¸°ë¡ 10ì‹œê°„ ì´ˆê³¼ ì œì™¸ ë˜ëŠ” í•„í„°ë¡œ ì¸í•´ ê³µì§‘í•©)")
        else:
            # 1) ë‹¨ì¼ ê¸°ë¡ ê¸°ì¤€ 'ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì‹œê°„' ì œì™¸ â€” í•œ ì‘ì—…ë‹¹ 10ì‹œê°„(600ë¶„) ì´ˆê³¼ ì œì™¸
            # (ì£¼ì˜) ì•„ë˜ í•œ ì¤„ì€ ì´ë¯¸ ì „ì—­ì— ABNORMAL_TASK_MIN = 600ì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ ì¤‘ë³µ ì •ì˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            #         ì „ì—­ ì •ì˜ê°€ ì—†ë‹¤ë©´, ë‹¤ìŒ ì¤„ì„ í™œì„±í™”í•˜ì„¸ìš”(ì£¼ì„ í•´ì œ).
            # ABNORMAL_TASK_MIN = 600  # 10ì‹œê°„(ë¶„)

            util_df = df[(df['ì‘ì—…ì‹œê°„(ë¶„)'] >= 0) & (df['ì‘ì—…ì‹œê°„(ë¶„)'] < ABNORMAL_TASK_MIN)].copy()

            # 2) ê°™ì€ 'ì—…ë¬´(êµ¬ë¶„)' ë‚´ì—ì„œë§Œ ì‹œê°„ ê²¹ì¹¨ ë³‘í•© â€” ì™„ì „ ë²¡í„°í™”(ëŒ€ìš©ëŸ‰ ìµœì í™”)
            grp_keys = ['íŒ€', 'ì‘ì—…ì', 'êµ¬ë¶„', 'ì‘ì—…ì¼', 'ì£¼ì°¨_í‘œì‹œ']
            tmp = (
                util_df[grp_keys + ['ì‹œì‘ì¼ì‹œ', 'ì¢…ë£Œì¼ì‹œ']]
                .dropna(subset=['ì‹œì‘ì¼ì‹œ', 'ì¢…ë£Œì¼ì‹œ'])
                .sort_values(grp_keys + ['ì‹œì‘ì¼ì‹œ'])
                .copy()
            )
            tmp['ì¢…ë£Œ_cummax'] = tmp.groupby(grp_keys)['ì¢…ë£Œì¼ì‹œ'].cummax()
            prev_cummax = tmp.groupby(grp_keys)['ì¢…ë£Œ_cummax'].shift()
            tmp['ìƒˆêµ¬ê°„'] = prev_cummax.isna() | (tmp['ì‹œì‘ì¼ì‹œ'] > prev_cummax)
            tmp['ì„¸ê·¸'] = tmp.groupby(grp_keys)['ìƒˆêµ¬ê°„'].cumsum()
            segments = (
                tmp.groupby(grp_keys + ['ì„¸ê·¸'])
                   .agg(seg_start=('ì‹œì‘ì¼ì‹œ', 'min'), seg_end=('ì¢…ë£Œì¼ì‹œ', 'max'))
                   .reset_index()
            )
            segments['ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)'] = (segments['seg_end'] - segments['seg_start']).dt.total_seconds() / 60.0
            merged = (
                segments.groupby(grp_keys, as_index=False)['ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)']
                        .sum()
            )

            # 3) íŒ€Ã—ì£¼ì°¨ë³„ ì£¼ê°„ ì‘ì—…ì‹œê°„ í•©(ë¶„)
            team_time = (
                merged.groupby(['íŒ€','ì£¼ì°¨_í‘œì‹œ'], as_index=False)['ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)']
                      .sum()
                      .rename(columns={'ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)':'ì£¼ê°„ì‘ì—…ì‹œê°„_í•©(ë¶„)'})
            )

            # ì£¼ì°¨ë³„ ì˜ì—…ì¼ìˆ˜(B, í‰ì¼) ê³„ì‚° â€” í™”ë©´ ì£¼ì°¨ í‘œì‹œ ì²´ê³„ì™€ ë™ì¼í•˜ê²Œ ë¼ë²¨ë§
            biz_days = pd.date_range(start=df['ì‘ì—…ì¼'].min(), end=df['ì‘ì—…ì¼'].max(), freq='B')
            cal = pd.DataFrame({'ì‘ì—…ì¼': biz_days})
            if week_mode == "ì›”ë‚´ì£¼ì°¨":
                cal['ì£¼ì°¨_í‘œì‹œ'] = cal['ì‘ì—…ì¼'].apply(lambda x: f"{x.month}ì›”{x.day // 7 + 1}ì£¼")
            else:
                cal['ì£¼ì°¨_í‘œì‹œ'] = pd.to_datetime(cal['ì‘ì—…ì¼']).dt.strftime('%G-W%V')
            bdf = cal.groupby('ì£¼ì°¨_í‘œì‹œ')['ì‘ì—…ì¼'].nunique().reset_index(name='ì˜ì—…ì¼ìˆ˜')

            # íŒ€ ì¸ì›(ì „ì²´) â€” ê°œì¸ë³„ ëˆ„ë½ë¥  â‰¥50% ì‘ì—…ìëŠ” ì œì™¸ (ì‘ì„±ì—¬ë¶€ mean<=0.5)
            try:
                valid_workers = _ps_all[_ps_all['mean'] > 0.5][['íŒ€','ì‘ì—…ì']].dropna().drop_duplicates()
                team_all = valid_workers.groupby('íŒ€')['ì‘ì—…ì'].nunique().reset_index(name='íŒ€ì¸ì›_ì „ì²´')
            except Exception:
                team_all = (
                    df[['íŒ€','ì‘ì—…ì']].dropna().drop_duplicates()
                      .groupby('íŒ€')['ì‘ì—…ì'].nunique().reset_index(name='íŒ€ì¸ì›_ì „ì²´')
                )

            # 4) ë³‘í•© ë° ê°€ë™ë¥ (ë¹„ìœ¨) ê³„ì‚° (0~1, ìƒí•œ 1.0)
            df_weekly = (
                team_time
                .merge(bdf, on='ì£¼ì°¨_í‘œì‹œ', how='left')
                .merge(team_all, on='íŒ€', how='left')
            )
            denom = (df_weekly['ì˜ì—…ì¼ìˆ˜'] * BASIS_MIN_PER_DAY * df_weekly['íŒ€ì¸ì›_ì „ì²´']).replace(0, np.nan)
            df_weekly['ê°€ë™ë¥ (%)'] = (df_weekly['ì£¼ê°„ì‘ì—…ì‹œê°„_í•©(ë¶„)'] / denom).clip(upper=1.0)
            df_weekly = df_weekly.sort_values(['íŒ€','ì£¼ì°¨_í‘œì‹œ']).reset_index(drop=True)

            team_count = df['íŒ€'].nunique()
            base_line = util_threshold / 100.0

            # ğŸ“ˆ ë©”íŠ¸ë¦­(ê°€ë™ë¥ )
            _util_avg = float(df_weekly['ê°€ë™ë¥ (%)'].mean()) if not df_weekly.empty else 0.0
            _team_avg = df_weekly.groupby('íŒ€')['ê°€ë™ë¥ (%)'].mean() if not df_weekly.empty else pd.Series(dtype=float)
            _team_above = int((_team_avg >= base_line).sum()) if not df_weekly.empty else 0
            _vals = (df_weekly['ê°€ë™ë¥ (%)'] * 100).replace([np.inf, -np.inf], np.nan).dropna()
            _util_p90 = int(np.nanpercentile(_vals, 90)) if (not df_weekly.empty and len(_vals) > 0) else 0
            u1, u2, u3 = st.columns(3)
            u1.metric("í‰ê·  ê°€ë™ë¥ ", f"{int(_util_avg*100)}%")
            u2.metric("ê¸°ì¤€ ì´ìƒ íŒ€ ìˆ˜", _team_above)
            u3.metric("ê°€ë™ë¥  P90", f"{_util_p90}%")

            # ì°¨íŠ¸
            fig_util = px.bar(
                df_weekly,
                x='íŒ€', y='ê°€ë™ë¥ (%)', color='ì£¼ì°¨_í‘œì‹œ', barmode='group',
                title='íŒ€ ì£¼ì°¨ë³„ ê°€ë™ë¥ ', labels={'ê°€ë™ë¥ (%)': 'ê°€ë™ë¥ ', 'íŒ€': 'íŒ€'}
            )
            fig_util.update_layout(
                yaxis_tickformat='.0%', yaxis_range=[0, 1],
                legend_title_text='ì£¼ì°¨', legend=dict(orientation='h', y=-0.2, x=0.5, xanchor='center')
            )
            fig_util.add_shape(
                type="line", x0=-0.5, x1=max(-0.5, team_count - 0.5),
                y0=base_line, y1=base_line, line=dict(color="red", width=2, dash="dot")
            )
            fig_util.add_annotation(
                x=max(-0.5, team_count - 0.5), y=base_line,
                text=f"ê¸°ì¤€: {util_threshold}%", showarrow=False, yshift=10, font=dict(color="red")
            )
            st.plotly_chart(fig_util, use_container_width=True)
            st.caption("â€» ê³„ì‚°ì‹: (ê°™ì€ 'ì—…ë¬´(êµ¬ë¶„)' ë‚´ ê²¹ì¹¨ ë³‘í•© í›„) ì£¼ê°„ì‘ì—…ì‹œê°„í•© Ã· (ì˜ì—…ì¼ìˆ˜ Ã— 480ë¶„ Ã— íŒ€ ì¸ì›(ì „ì²´, ëˆ„ë½ë¥ â‰¥50% ì œì™¸)) Ã— 100 Â· ë‹¨ì¼ ê¸°ë¡ 10ì‹œê°„ ì´ˆê³¼ ì œì™¸.")

        # ğŸ“Š ì¼ë³„ í‰ê·  ì‘ì—… ì‹œê°„ â€” 10ì‹œê°„ ì´ˆê³¼ ì œì™¸ + ê°™ì€ 'ì—…ë¬´(êµ¬ë¶„)' ë‚´ ê²¹ì¹¨ ë³‘í•© ì ìš©
        st.markdown("## ğŸ“Š ì¼ë³„ í‰ê·  ì‘ì—… ì‹œê°„")

# (ìë™ì •ë¦¬) ì¤‘ë³µ ì •ì˜ ì œê±°:         ABNORMAL_TASK_MIN = 600  # í•œ ì‘ì—…ë‹¹ 10ì‹œê°„(ë¶„) ì´ˆê³¼ ì œì™¸
        daily_src = df[(df['ì‘ì—…ì‹œê°„(ë¶„)'] >= 0) & (df['ì‘ì—…ì‹œê°„(ë¶„)'] < ABNORMAL_TASK_MIN)].copy()

        if daily_src.empty:
            st.info("ì¼ë³„ í‰ê·  ì‘ì—… ì‹œê°„ì„ ê³„ì‚°í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë‹¨ì¼ ê¸°ë¡ 10ì‹œê°„ ì´ˆê³¼ ì œì™¸ ë“±)")
            daily_sum = pd.DataFrame(columns=['ì‘ì—…ì¼', 'íŒ€', 'ì‘ì—…ì‹œê°„(ë¶„)'])
            daily_worker_count = pd.DataFrame(columns=['ì‘ì—…ì¼', 'íŒ€', 'ì‘ì—…ììˆ˜'])
        else:
            # ê°™ì€ 'ì—…ë¬´(êµ¬ë¶„)' ë‚´ì—ì„œ ê²¹ì¹¨ ë³‘í•© â€” ì™„ì „ ë²¡í„°í™”(ëŒ€ìš©ëŸ‰ ìµœì í™”)
            gkeys = ['íŒ€', 'ì‘ì—…ì', 'êµ¬ë¶„', 'ì‘ì—…ì¼']
            tmp = (
                daily_src[gkeys + ['ì‹œì‘ì¼ì‹œ', 'ì¢…ë£Œì¼ì‹œ']]
                .dropna(subset=['ì‹œì‘ì¼ì‹œ', 'ì¢…ë£Œì¼ì‹œ'])
                .sort_values(gkeys + ['ì‹œì‘ì¼ì‹œ'])
                .copy()
            )
            # ê·¸ë£¹ë³„ ì¢…ë£Œì‹œê° ëˆ„ì ìµœëŒ“ê°’ê³¼ ì´ì „ êµ¬ê°„ì˜ ëˆ„ì ìµœëŒ“ê°’
            tmp['ì¢…ë£Œ_cummax'] = tmp.groupby(gkeys)['ì¢…ë£Œì¼ì‹œ'].cummax()
            prev_cummax = tmp.groupby(gkeys)['ì¢…ë£Œ_cummax'].shift()
            # ìƒˆ êµ¬ê°„ ì‹œì‘ ì—¬ë¶€(ì´ì „ ì¢…ë£Œ ëˆ„ì ìµœëŒ“ê°’ë³´ë‹¤ ì‹œì‘ì´ ë’¤ë©´ ìƒˆ êµ¬ê°„)
            tmp['ìƒˆêµ¬ê°„'] = prev_cummax.isna() | (tmp['ì‹œì‘ì¼ì‹œ'] > prev_cummax)
            # ê·¸ë£¹ ë‚´ êµ¬ê°„ ë²ˆí˜¸
            tmp['ì„¸ê·¸'] = tmp.groupby(gkeys)['ìƒˆêµ¬ê°„'].cumsum()
            # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ [min ì‹œì‘, max ì¢…ë£Œ]
            segments = (
                tmp.groupby(gkeys + ['ì„¸ê·¸'])
                   .agg(seg_start=('ì‹œì‘ì¼ì‹œ', 'min'), seg_end=('ì¢…ë£Œì¼ì‹œ', 'max'))
                   .reset_index()
            )
            # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´(ë¶„) ê³„ì‚° í›„ ê·¸ë£¹ë³„ í•©
            segments['ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)'] = (segments['seg_end'] - segments['seg_start']).dt.total_seconds() / 60.0
            merged_daily = (
                segments.groupby(gkeys, as_index=False)['ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)']
                        .sum()
            )
            # íŒ€ë³„ ì¼ì í•©ê³„ (ì°¨íŠ¸/í‘œ ì…ë ¥ìš© DataFrame)
            daily_sum = (
                merged_daily
                .groupby(['ì‘ì—…ì¼', 'íŒ€'], as_index=False)['ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)']
                .sum()
                .rename(columns={'ë³‘í•©ì‘ì—…ì‹œê°„(ë¶„)': 'ì‘ì—…ì‹œê°„(ë¶„)'})
            )

            # ì¼ìë³„ íŒ€ ì‘ì—…ì ìˆ˜ (unique)
            daily_worker_count = (
                daily_src[['ì‘ì—…ì¼', 'íŒ€', 'ì‘ì—…ì']]
                .dropna()
                .drop_duplicates()
                .groupby(['ì‘ì—…ì¼', 'íŒ€'])['ì‘ì—…ì']
                .nunique()
                .reset_index(name='ì‘ì—…ììˆ˜')
            )

        # í‰ê·  ê³„ì‚° ë° ì‹œê°í™”
        daily_avg = daily_sum.merge(daily_worker_count, on=['ì‘ì—…ì¼', 'íŒ€'], how='inner')
        daily_avg = daily_avg.replace([np.inf, -np.inf], np.nan).dropna(subset=['ì‘ì—…ììˆ˜'])
        daily_avg['í‰ê· ì‘ì—…ì‹œê°„(ì‹œê°„)'] = (daily_avg['ì‘ì—…ì‹œê°„(ë¶„)'] / daily_avg['ì‘ì—…ììˆ˜']) / 60

        # ğŸ“ˆ ë©”íŠ¸ë¦­(ì¼ë³„ í‰ê·  ì‘ì—… ì‹œê°„)
        _mean_hours = float(daily_avg['í‰ê· ì‘ì—…ì‹œê°„(ì‹œê°„)'].mean()) if not daily_avg.empty else 0.0
        _exceed_cnt = int((daily_avg['í‰ê· ì‘ì—…ì‹œê°„(ì‹œê°„)'] >= daily_avg_threshold_hours).sum()) if not daily_avg.empty else 0
        _max_hours = float(daily_avg['í‰ê· ì‘ì—…ì‹œê°„(ì‹œê°„)'].max()) if not daily_avg.empty else 0.0
        d1, d2, d3 = st.columns(3)
        d1.metric("í‰ê· (ì‹œê°„)", f"{_mean_hours:.1f}")
        d2.metric("ê¸°ì¤€ ì´ˆê³¼ ê±´ìˆ˜", _exceed_cnt)
        d3.metric("ìµœëŒ€ í‰ê· ì‹œê°„", f"{_max_hours:.1f}")

        fig_daily = px.bar(
            daily_avg,
            x='íŒ€',
            y='í‰ê· ì‘ì—…ì‹œê°„(ì‹œê°„)',
            color='ì‘ì—…ì¼',
            barmode='group',
            title='ì¼ë³„ í‰ê·  ì‘ì—… ì‹œê°„',
            labels={'í‰ê· ì‘ì—…ì‹œê°„(ì‹œê°„)': 'í‰ê·  ì‘ì—… ì‹œê°„(ì‹œê°„)', 'ì‘ì—…ì¼': 'ë‚ ì§œ'}
        )
        ymax = max(10.0, float(daily_avg_threshold_hours) * 1.2)
        fig_daily.update_layout(
            yaxis_range=[0, ymax],
            legend=dict(orientation='h', y=-0.25, x=0.5, xanchor='center')
        )
        _x1 = max(-0.5, len(daily_avg['íŒ€'].unique()) - 0.5)
        fig_daily.add_shape(
            type="line",
            x0=-0.5, x1=_x1,
            y0=daily_avg_threshold_hours, y1=daily_avg_threshold_hours,
            line=dict(color="red", width=2, dash="dot")
        )
        fig_daily.add_annotation(
            x=_x1,
            y=daily_avg_threshold_hours,
            text=f"ê¸°ì¤€: {daily_avg_threshold_hours:.1f}ì‹œê°„",
            showarrow=False,
            yshift=10,
            font=dict(color="red")
        )
        st.plotly_chart(fig_daily, use_container_width=True)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # â€ ğŸ‘·â€íŒ€ë³„ ìš´ìš©ì¡° í˜„í™©
        st.markdown("## ğŸ‘· íŒ€ë³„ ìš´ìš©ì¡° í˜„í™©")
        crew_base = df.groupby(['íŒ€', 'ì›ë³¸ì‘ì—…ì']).first().reset_index()
        crew_base['ì¡°êµ¬ì„±'] = crew_base['ì›ë³¸ì‘ì—…ì'].apply(lambda x: '2ì¸ 1ì¡°' if len(split_workers(x)) >= 2 else '1ì¸ 1ì¡°')
        crew_summary = crew_base.groupby(['íŒ€', 'ì¡°êµ¬ì„±']).size().unstack(fill_value=0)
        crew_summary_percent = crew_summary.div(crew_summary.sum(axis=1), axis=0).fillna(0).round(4) * 100
        # ğŸ“ˆ ë©”íŠ¸ë¦­(íŒ€ë³„ ìš´ìš©ì¡°)
        try:
            _avg_one = float(crew_summary_percent['1ì¸ 1ì¡°'].mean()) if '1ì¸ 1ì¡°' in crew_summary_percent.columns else 0.0
            _avg_two = float(crew_summary_percent['2ì¸ 1ì¡°'].mean()) if '2ì¸ 1ì¡°' in crew_summary_percent.columns else 0.0
        except Exception:
            _avg_one, _avg_two = 0.0, 0.0
        _team_n = int(crew_summary_percent.shape[0])
        c1, c2, c3 = st.columns(3)
        c1.metric("í‰ê·  1ì¸ 1ì¡° ë¹„ìœ¨", f"{int(round(_avg_one))}%")
        c2.metric("í‰ê·  2ì¸ 1ì¡° ë¹„ìœ¨", f"{int(round(_avg_two))}%")
        c3.metric("íŒ€ ìˆ˜", _team_n)
        st.dataframe(
            crew_summary_percent.T.style.format("{:.2f}%"),
            use_container_width=True
        )

        crew_summary_reset = crew_summary_percent.reset_index().melt(id_vars='íŒ€', var_name='ì¡°êµ¬ì„±', value_name='ë¹„ìœ¨')
        fig_crew = px.bar(
            crew_summary_reset,
            x='íŒ€',
            y='ë¹„ìœ¨',
            color='ì¡°êµ¬ì„±',
            color_discrete_map={'1ì¸ 1ì¡°': '#1f77b4', '2ì¸ 1ì¡°': '#ff7f0e'},
            barmode='group',
            title='íŒ€ë³„ ìš´ìš©ì¡° í˜„í™©',
            labels={'ë¹„ìœ¨': 'ë¹„ìœ¨(%)'}
        )
        fig_crew.update_layout(
            yaxis_range=[0, 100],
            yaxis_ticksuffix="%",
            legend=dict(orientation='h', y=-0.2, x=0.5, xanchor='center')
        )
        st.plotly_chart(fig_crew, use_container_width=True)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # âœ… ì—…ë¬´êµ¬ë¶„ë³„ ì¸ì›ì¡° í˜„í™©
        df_taskcrew = df.drop_duplicates(
            subset=['íŒ€', 'ì›ë³¸ì‘ì—…ì', 'ì‹œì‘ì¼ì‹œ', 'ì¢…ë£Œì¼ì‹œ', 'êµ¬ë¶„']
        ).copy()
        df_taskcrew['ì‘ì—…ìëª©ë¡'] = df_taskcrew['ì›ë³¸ì‘ì—…ì'].apply(split_workers)
        df_taskcrew['ì¡°êµ¬ì„±'] = df_taskcrew['ì‘ì—…ìëª©ë¡'].apply(
            lambda x: '2ì¸ 1ì¡°' if len(x) >= 2 else '1ì¸ 1ì¡°'
        )
        # ğŸ‘ ì—¬ê¸°ì„œëŠ” explode ë¶ˆí•„ìš” â€” ì‘ì—… 1ê±´ë‹¹ ì¡°êµ¬ì„± 1ê°œë§Œ ë°˜ì˜

        crew_task = df_taskcrew[['êµ¬ë¶„', 'ì¡°êµ¬ì„±']].copy()
        crew_task_grouped = crew_task.groupby(['êµ¬ë¶„', 'ì¡°êµ¬ì„±']).size().unstack(fill_value=0)
        crew_task_ratio = crew_task_grouped.div(crew_task_grouped.sum(axis=1), axis=0).fillna(0).round(4) * 100
        # ğŸ“ˆ ë©”íŠ¸ë¦­(ì—…ë¬´êµ¬ë¶„ë³„ ì¸ì›ì¡°)
        try:
            _avg_two_task = float(crew_task_ratio['2ì¸ 1ì¡°'].mean()) if '2ì¸ 1ì¡°' in crew_task_ratio.columns else 0.0
            _avg_one_task = float(crew_task_ratio['1ì¸ 1ì¡°'].mean()) if '1ì¸ 1ì¡°' in crew_task_ratio.columns else 0.0
        except Exception:
            _avg_two_task, _avg_one_task = 0.0, 0.0
        _task_n = int(crew_task_ratio.shape[0])
        t1, t2, t3 = st.columns(3)
        t1.metric("í‰ê·  2ì¸ 1ì¡° ë¹„ìœ¨", f"{int(round(_avg_two_task))}%")
        t2.metric("í‰ê·  1ì¸ 1ì¡° ë¹„ìœ¨", f"{int(round(_avg_one_task))}%")
        t3.metric("ì—…ë¬´ êµ¬ë¶„ ìˆ˜", f"{_task_n}")
        crew_task_reset = crew_task_ratio.reset_index().melt(id_vars='êµ¬ë¶„', var_name='ì¡°êµ¬ì„±', value_name='ë¹„ìœ¨')
        fig_crew_task = px.bar(
            crew_task_reset,
            x='êµ¬ë¶„',
            y='ë¹„ìœ¨',
            color='ì¡°êµ¬ì„±',
            color_discrete_map={'1ì¸ 1ì¡°': '#1f77b4', '2ì¸ 1ì¡°': '#ff7f0e'},
            barmode='group',
            title='ì—…ë¬´êµ¬ë¶„ë³„ ì¸ì›ì¡° í˜„í™©',
            labels={'ë¹„ìœ¨': 'ë¹„ìœ¨(%)'}
        )
        fig_crew_task.update_layout(
            yaxis_range=[0, 100],
            yaxis_ticksuffix="%",
            legend=dict(orientation='h', y=-0.2, x=0.5, xanchor='center')
        )
        st.plotly_chart(fig_crew_task, use_container_width=True)

        # ğŸ“Š ê²°ê³¼ ë¬¶ìŒ ë‹¤ìš´ë¡œë“œ (XLSX ë˜ëŠ” ZIP ìë™)
        excel_bytes, excel_fmt = to_excel({
            'ì¤‘ë³µì¶œë™': dup_display.reset_index(drop=True),
            'ê°œì¸ë³„ëˆ„ë½': table_df.reset_index(drop=True),
            'MTTR_ë°˜ë³µë„': result_display.reset_index(drop=True),
            'ì¼ë³„ì‘ì„±í˜„í™©': daily_count.rename_axis(index='ì‘ì—…ì¼').reset_index(),
            'ì‘ì—…ì‹œê°„ì˜¤ë¥˜_ì„ê³„': long_rows.reset_index(drop=True),
            'ì‘ì—…ì‹œê°„ì˜¤ë¥˜_0ë¶„': zero_rows.reset_index(drop=True),
        })
        if excel_fmt == "xlsx":
            fname = f"ì—…ë¬´ì¼ì§€_ë¶„ì„_ëª¨ìŒ_{team_tag}_{date_tag}.xlsx"
            mime = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            label = "ğŸ“Š Excel(ëª¨ë“  ì‹œíŠ¸)"
        else:
            fname = f"ì—…ë¬´ì¼ì§€_ë¶„ì„_ëª¨ìŒ_{team_tag}_{date_tag}.zip"
            mime = "application/zip"
            label = "ğŸ“¦ ZIP(ëª¨ë“  CSV)"
        st.sidebar.download_button(label, data=excel_bytes, file_name=fname, mime=mime)


if __name__ == '__main__':
    main()